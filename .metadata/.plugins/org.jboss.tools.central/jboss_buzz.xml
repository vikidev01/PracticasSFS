<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Support any JSON, metrics and CSV as dataset in Dashbuilder</title><link rel="alternate" href="https://blog.kie.org/2022/04/support-any-json-metrics-and-csv-as-dataset-in-dashbuilder.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/04/support-any-json-metrics-and-csv-as-dataset-in-dashbuilder.html</id><updated>2022-04-29T20:14:56Z</updated><content type="html">A few weeks ago we released . At that time a limited type of JSON was supported, now external datasets are more powerful than ever supporting any JSON format, CSV and metrics.  External datasets are the source of data that are outside Dashbuilder, hence they can be loaded on client side, removing the requirement of a backend to run dashbuilder. In this post we will share all the new features added to external datasets. By the way, you can try the new features in a quick-try . EXTERNAL DATASETS IMPROVEMENTS External datasets now support the following new features: * Headers: It is possible to declare headers. This is important to provide bearer tokens or any custom header * Columns: As stated in the announcement post, column information can be provided on the JSON itself otherwise dashbuilder “guess” the type for the columns and give it a generic name. Now it is possible to declare the columns to override the values coming from the JSON. * Transformation: Most of the cases the JSON you want to use will not be in the formats supported by Dashbuilder. To overcome this we now support transform expressions. Our selected tool is , but there are discussions to support in future versions. The transform expression can be provided using the field expression. The result expression will be applied on the input JSON and the result must be a JSON two dimension array. To find the right expression for your JSON you can use the .  More than simply transforming the input to an array, the support to expressions is also important to cover edge cases of a source and apply complex changes to the original source. Check for more information. * Inline JSON: You may not have access to the dataset during development. In this case you can use the new content field to declare an inline JSON two dimensional array that will serve as the dataset. NEW SUPPORTED FORMATS JSON is already supported by external datasets, and to extend the power of external datasets we introduced two new formats that are supported on the client (without the requirement of a backend): * CSV: Dashbuilder now has experimental support for CSV as the external data set. The only format it supports is the specified in with the following limitations: * The first line, supposedly the header, is always skipped * Fields with line breaks are not supported * Metrics: Nowadays Prometheus is the standard for keeping track of metrics of an application. It is possible to grab data directly from Prometheus REST API Using JSON and transformation. There are cases where the service is not connected to Prometheus and users may want to read directly from the metrics source, for this reason we transform content from URLs ending with “metrics” as a dataset, which makes it possible to read metrics without the need of a Prometheus installation. Bear in mind that it only reads a snapshot of the metrics, to keep the history of metrics you will still need a Prometheus. All these new external dataset formats are internally handled as a JSON two dimensional array, which means that you can still transform it using JSONata. CONCLUSION In this post we shared the new features available in the latest Dashbuilder release. Soon we will share real data visualizations and dashboards using popular APIs. Stay tuned! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title type="html">Serverless Workflow Expressions</title><link rel="alternate" href="https://blog.kie.org/2022/04/serverless-workflow-expressions.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2022/04/serverless-workflow-expressions.html</id><updated>2022-04-28T19:27:15Z</updated><content type="html">Expressions are an essential feature of . They are everywhere and they are powerful. As you should already be aware if you have ever watched a superhero movie, with . In the universe, when discussing expressions, this famous sentence means there is a risk you will overuse them. Or, employing culinary terms, expressions are like the salt in a stew, you need to find out the right amount for your recipe to excel.   But what is exactly an expression? In a nutshell, a string that adheres to certain conventions established by a language specification that allows you to interact with the flow model. There are two terms in the previous sentence that deserve explanation: workflow data model and language specification. Every workflow instance is associated with a data model. This model, regardless if your flow file is written in or , consists of a JSON object.  The initial content of that object is set by the creator of the flow. As you already know, the flow can be started through a or a POST invocation. No matter which approach you use to start the flow, both the event or the request body will be a JSON object, which is expected to have a workflowdata property. Its value, typically another JSON object, will be used as the initial model. That model will be accessed/updated as part of the flow execution. Expressions are the mechanism defined by the for the states to interact with the model. Kogito supports two expression languages: and . The default one is jq, but you can change it to jsonpath by using the expressionLang property. Why these two languages? As you already guessed, since the flow model is a JSON object, it makes sense that the expression languages intended to interact with it are JSON-oriented ones. Why is jq the default? Because it is more powerful. In fact, jsonpath is not suitable for all use cases supported by the specification, as you will soon find out in this post.   Given this quick introduction, let’s discuss in this post some use cases for expressions: switch state conditions, action function args and state filtering.  SWITCH CONDITIONS Unlike a human, a flow does not have free will, its destiny is decided by the contents of the model and the flow designer. Conditions inside a switch state allow the flow designer to choose which path the flow should follow depending on the model content. A condition is an expression that returns a boolean when evaluated against the model. If the condition associated with a state transition returns true, that is the place for the flow to go.  For example, in repository, we are selecting which message should be displayed to the user depending on his language of choice: English or Spanish. In computational terms, if the value of the property language is english, the constant literal to be injected  on property message will be Hello from… , else if the value of the same property is spanish, then the injected message will be Saludos desde…. Using jq as expression language and JSON as workflow definition language, the switch state contains.  "dataConditions": [         {           "condition": "${ .language == \"English\" }",           "transition": "GreetInEnglish"         },         {           "condition": "${ .language == \"Spanish\" }",           "transition": "GreetInSpanish"         }       ] Using jsonpath as expression language and YAML as workflow definition language, the switch would have look like  dataConditions:       - condition: "${$.[?(@.language  == 'English')]}"         transition: GreetInEnglish       - condition: "${$.[?(@.language  == 'Spanish')]}"         transition: GreetInSpanish Note that, as required by the specification, in these examples, all expressions are embedded within ${… }.  But Kogito is smart enough to infer that the string within condition is an expression, so you can skip it and just write "dataConditions": [         {           "condition": ".language == \"English\"",           "transition": "GreetInEnglish"         },         {           "condition": ".language == \"Spanish\"",           "transition": "GreetInSpanish"         }] Which will behave the same and is a bit shorter.  FUNCTION ARGUMENTS One of the coolest things about Serverless Workflow Specification is the capability to define that can be invoked several times by the states of the flow. Every different function call might contain different arguments, which are specified using function arguments. A function definition example can be found in the repository. This flow performs two consecutive REST invocations to convert Fahrenheit to Celsius (a subtraction and a multiplication). Lets focus on the first function definition, the subtraction.  "functions": [ {   "name": "subtraction",   "operation": "specs/subtraction.yaml#doOperation" }] In the snippet above, we are defining a function called subtraction that performs an call. Kogito knows that the operation property defines an OpenAPI invocation because REST is the default operation type (adding “type”: “rest” will also work, but it is redundant). The OpenAPI specification URI is the sub-string before the # character in the operation property. The operationId is the sub-string after the #. In Kogito Serverless Workflow implementation, when an URI does not have a scheme, it is assumed to be located in the classpath of the Maven project. paths: /: post: operationId: doOperation requestBody: content: application/json: schema: $ref: '#/components/schemas/SubtractionOperation' responses: "200": description: OK components: schemas: SubtractionOperation: type: object properties: leftElement: format: float type: number rightElement: format: float type: number As you can see in the snippet above, subtraction.yaml specification file referenced in this example defines an operationId called doOperation, which expects two parameters: leftelement and rightelement. In order to invoke a function, we use a construct. It is composed by the refName (which should match the function definition name) and the arguments to be used in the function call.  Function arguments are expressed as a JSON object whose property values might be either a string containing an expression or any (string, number, boolean…). Note that in this example, the expression is not embedded within ${}. Kogito will infer it is a valid jq expression because of the . prefix, but you can embed it if you prefer to do so.  "functionRef": { "refName": "subtraction", "arguments": { "leftElement": ".fahrenheit", "rightElement": ".subtractValue" } } In the snippet above, we are specifying that the left number of the subtraction is equal to fahrenheit property (which is an input number provided by the user invoking the flow) and that the right element is equal to substractvalue property (which is a constant number injected into the flow model by SetConstants state). After resolving expression evaluation, the JSON object is used as a request body. "functionRef": { "refName": "subtraction", "arguments": "{leftElement: .fahrenheit, rightElement : .subtractValue}" } You can also write function arguments as a string containing an expression that returns a JSON object. You should be aware that this capability might not be supported by the expression language (jsonpath does not, hence one of the reasons why jq is the default expression language in the specification). Also, it is only suitable when the OpenAPI operation does not define any path, query or header parameter. The snippet above returns the same JSON object as in the previous one, but using a jq expression string rather than a JSON object. SETTING OPENAPI PARAMETERS In the previous example, the function arguments are used as the POST/PUT request body. But what happens if you want to set path, header or query parameters? In this case, you must use the JSON object approach, where the proper query, path or header parameters are extracted from arguments matching the name of the parameter. The remaining arguments in the JSON object, if any, will be used as  the request body.  Let’s illustrate it with an example, consider the following OpenAPI definition, which adds a header named pepe to multiplication operation id. paths: /: post: operationId: doOperation parameters: - in: header name: pepe schema: type: string required: false requestBody: content: application/json: schema: $ref: '#/components/schemas/MultiplicationOperation' responses: "200": description: OK components: schemas: MultiplicationOperation: type: object properties: leftElement: format: float type: number rightElement: format: float type: number You can set the value for header pepe by including a property named pepe into the function arguments using the JSON object approach, as in the snippet below, which sets the value of the header to pepa. As explained before, the resulting POST request body will just contain leftElement and rightElement. "functionRef": { "refName": "multiplication", "arguments": { "pepe":"pepa", "leftElement": ".subtraction.difference", "rightElement": ".multiplyValue" } } STATE FILTERING Let’s conclude our review of expression usage by considering this . The input model is an array of complex numbers (where x is the real coordinate and y the imaginary one) and the output model is the maximum value of the real coordinate within the input array. The flow consists of an action expression (defined inside squareState) that calculates the maximum x and the minimum y; an (defined inside squareState) that selects the maximum x as the action output that should be merged into the model; and a (defined inside finish state) that sets that max value as the whole model that will be returned to the caller. Let’s examine the three of them in detail.  "functions": [ { "name": "max", "type": "expression", "operation": "{max: .numbers | max_by(.x), min: .numbers | min_by(.y)}" } ] In the snippet above we define a max function of type expression. The operation property is a string containing a jq expression. This expression returns a JSON object, where max property is the maximum value of x coordinate in the input array and min property is the minimum value of y coordinate in the same array.  Now let’s go to the place where this function is invoked. "actions": [ { "name": "maxAction", "functionRef": { "refName": "max" }, "actionDataFilter": { "results" : ".max.x", "toStateData" : ".number" } } ] Since we are only interested in the maximum x, besides invoking the function using functionRef, we add an action data filter. If we do not add this filter, the whole JSON Object returned by the function call will be merged into the flow model. The filter has two properties: results, which selects the attribute and toStateData, which indicates the name of the target property inside the flow model (in case this property does not exist, it will be added). So, after executing the action, the flow model will consist of a number property storing the maximum value and the original numbers array. Then the flow moves to the next state: finish. "name": "finish", "type": "operation", "stateDataFilter": { "input": "{result: .number}" } Since we do not want to return the user input as a result of the flow execution, the final stage consists of a state data filter that sets the contents of the output model. Hence, we set the model to be a JSON object containing a property named result, whose value is the maximum number calculated by the previous state, stored in the number property. We do this  using the input property of the stateDataFilter construct, meaning that the model is changed before the state gets executed. So the final model content returned to the user contains a result property whose value is the maximum x. There are some aspects I would like to remark before jumping to the conclusions:  * The significant difference between action and state data filters. While the former just selects the portion of the action result that will be merged into the model, overriding only those properties in the flow model that share the name with the selected action result, the latter sets the whole flow model to the JSON object returned by the expression, discarding any existing property.  * Since stateDataFilter is expected to return a whole JSON object, jq is the only real valid option if you are using this construct (remember jsonpath expressions are not able to build new JSON objects) * We use expression strings that are not embedded within ${..}. As explained before, this is just syntactic sugar. You can use the approach you prefer. CONCLUSION This post illustrates usage of expressions in three main areas of Serverless Workflow specification: branching using Switch State, invoking OpenAPI services with arguments that are extracted from the flow model, and manipulating the flow model using jq expression language. Now is your turn to combine the three of them to perform complex and unrestricted service orchestration. The only limit is your imagination. The post appeared first on .</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator></entry><entry><title>Use Red Hat's single sign-on technology to secure services through Kerberos</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/28/use-red-hats-single-sign-technology-secure-services-through-kerberos" /><author><name>Rishabh Singh</name></author><id>4bd743c2-cdfd-40e9-ba42-4973b71428a4</id><updated>2022-04-28T07:00:00Z</updated><published>2022-04-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat's single sign-on technology&lt;/a&gt; is an identity and access management solution based on standard identity protocols (SAML, OpenID Connect) to perform authentication of users and share user information for access control. Red Hat's SSO sources user information from a &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.5/html/server_administration_guide/user-storage-federation"&gt;federated user database&lt;/a&gt;, or &lt;em&gt;user federation,&lt;/em&gt; and it provides the option to configure the &lt;a href="https://web.mit.edu/kerberos/"&gt;Kerberos protocol&lt;/a&gt; for this purpose.&lt;/p&gt; &lt;p&gt;In this article, you'll see how to set up Red Hat's SSO to authenticate users using the standard Kerberos protocol along with the &lt;a href="https://www.ietf.org/rfc/rfc2478.txt"&gt;Simple and Protected GSS_API Negotiation Mechanism&lt;/a&gt; (SPNEGO) specification.&lt;/p&gt; &lt;p&gt;To use Kerberos, Red Hat's SSO must set up an identity called a &lt;em&gt;service principal&lt;/em&gt;. The user gains access to Red Hat's SSO through Kerberos in a two-step process: first they obtain a Ticket Granting Ticket (TGT) and then they obtain service tickets (ST).&lt;/p&gt; &lt;p&gt;Objects in the Kerberos key distribution center (KDC) database are known as &lt;em&gt;principals.&lt;/em&gt; Each principal is a user, service, or host. For this example, you will add a user principal and an HTTP service principal.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates the flow of information and tokens.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_4.png?itok=G7hGU7Cg" width="781" height="588" alt="A client authenticates both with the Kerberos KDC and with Red Hat's SSO." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. A client authenticates both with the Kerberos KDC and with Red Hat's SSO. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: A client authenticates both with the Kerberos KDC and with Red Hat's SSO.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow this example, you need at least two &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; servers sharing a network and a domain. The example uses &lt;code&gt;s1.example.com&lt;/code&gt; and &lt;code&gt;s2.example.com&lt;/code&gt; as the names for the systems, but you'll want to substitute in yours. Red Hat's single sign-on technology should be installed on &lt;code&gt;s2.example.com&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We've used Mozilla Firefox in the example screenshots to display the services and authenticate with Red Hat's single sign-on servers.&lt;/p&gt; &lt;h2&gt;Summary of steps in this article&lt;/h2&gt; &lt;p&gt;The procedure we'll go through breaks down as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Set up the Kerberos server &lt;ol&gt; &lt;li&gt;Configure the Kerberos server&lt;/li&gt; &lt;li&gt;Add principals and the export keytab&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Set up Red Hat's SSO server &lt;ol&gt; &lt;li&gt;Configure the service principal and keytab file&lt;/li&gt; &lt;li&gt;Enable Kerberos processing&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Set up your client machine &lt;ol&gt; &lt;li&gt;Configure the Kerberos client&lt;/li&gt; &lt;li&gt;Enable logins through SPNEGO&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Authenticate to Kerberos&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Set up the Kerberos server&lt;/h2&gt; &lt;p&gt;In this section, you'll install and configure a Kerberos server. This server will be used later to create service tickets that authenticate users to Red Hat's SSO.&lt;/p&gt; &lt;p&gt;We are using EXAMPLE.COM as our Kerberos realm. The Kerberos server (KDC) will be running on the host &lt;code&gt;s1.example.com&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Configure the Kerberos server&lt;/h3&gt; &lt;p&gt;As an administrator on your system, enter the following commands.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Install the Kerberos server and relevant libraries on &lt;code&gt;s1.example.com&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# yum install krb5-server krb5-workstation sssd*&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Review the &lt;code&gt;/etc/krb5.conf&lt;/code&gt; and &lt;code&gt;/var/kerberos/krb5kdc/kdc.conf&lt;/code&gt; files. Configure the realms as EXAMPLE.COM, and &lt;code&gt;kdc&lt;/code&gt; and &lt;code&gt;admin_server&lt;/code&gt; as &lt;code&gt;s1.example.com&lt;/code&gt;, as follows. Here's&lt;code&gt;kdc.conf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88 spake_preauth_kdc_challenge = edwards25519 [realms] EXAMPLE.COM = { #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal arcfour-hmac:normal camellia256-cts:normal camellia128-cts:normal &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And here's &lt;code&gt;krb5.conf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false pkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crt spake_preauth_groups = edwards25519 default_realm = EXAMPLE.COM default_ccache_name = KEYRING:persistent:%{uid} [realms] EXAMPLE.COM = { kdc = s1.example.com admin_server = s1.example.com } [domain_realm] .example.com = EXAMPLE.COM example.com = EXAMPLE.COM &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Put the following line in your &lt;code&gt;/var/kerberos/krb5kdc/kadm5.acl&lt;/code&gt; file, so that you can get access to the EXAMPLE.COM domain as an administrator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;*/admin@EXAMPLE.COM *&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create the KDC database using the &lt;code&gt;kdb5_util&lt;/code&gt; utility:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# kdb5_util create -s -r EXAMPLE.COM&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Start and enable the Kerberos services:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# systemctl start krb5kdc kadmin # systemctl enable krb5kdc kadmin&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create the first principal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# kadmin.local -q "addprinc root/admin" [root@s1 ~]# systemctl start krb5kdc kadmin [root@s1 ~]# [root@s1 ~]# kadmin.local Authenticating as principal root/admin@EXAMPLE.COM with password. kadmin.local: addprinc root/admin No policy specified for root/admin@EXAMPLE.COM; defaulting to no policy Enter password for principal "root/admin@EXAMPLE.COM": Re-enter password for principal "root/admin@EXAMPLE.COM": Principal "root/admin@EXAMPLE.COM" created. kadmin.local: addprinc rishabh No policy specified for rishabh@EXAMPLE.COM; defaulting to no policy Enter password for principal "rishabh@EXAMPLE.COM": Re-enter password for principal "rishabh@EXAMPLE.COM": Principal "rishabh@EXAMPLE.COM" created. kadmin.local: &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Add principals and the export keytab&lt;/h3&gt; &lt;p&gt;As mentioned earlier, a KDC principal can be a user, service, or host. For this example, you will add the user principal and HTTP service principal.&lt;/p&gt; &lt;p&gt;The user principal will be used to authenticate to and get access to the account management console on Red Hat's SSO.&lt;/p&gt; &lt;p&gt;The key of the HTTP service principal will be exported to a keytab file. Later, the HTTP service principal and exported keytab will be used to configure Kerberos-based user federation using Red Hat's SSO.&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;Add the user principal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# kadmin.local kadmin.local: addprinc rishabh kadmin.local: quit&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add the HTTP service principal and export keytab:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kadmin.local: addprinc -randkey HTTP/s2.example.com@EXAMPLE.COM No policy specified for HTTP/s2.example.com@EXAMPLE.COM; defaulting to no policy Principal "HTTP/s2.example.com@EXAMPLE.COM" created. kadmin.local: ktadd -norandkey -k /etc/jboss_s2_Example.keytab HTTP/s2.example.com@EXAMPLE.COM Entry for principal HTTP/s2.example.com@EXAMPLE.COM with kvno 1, encryption type aes256-cts-hmac-sha1-96 added to keytab WRFILE:/etc/jboss_s2_Example.keytab. Entry for principal HTTP/s2.example.com@EXAMPLE.COM with kvno 1, encryption type aes128-cts-hmac-sha1-96 added to keytab WRFILE:/etc/jboss_s2_Example.keytab. Entry for principal HTTP/s2.example.com@EXAMPLE.COM with kvno 1, encryption type arcfour-hmac added to keytab WRFILE:/etc/jboss_s2_Example.keytab. Entry for principal HTTP/s2.example.com@EXAMPLE.COM with kvno 1, encryption type camellia256-cts-cmac added to keytab WRFILE:/etc/jboss_s2_Example.keytab. Entry for principal HTTP/s2.example.com@EXAMPLE.COM with kvno 1, encryption type camellia128-cts-cmac added to keytab WRFILE:/etc/jboss_s2_Example.keytab. kadmin.local: &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Set up Red Hat's SSO server&lt;/h2&gt; &lt;p&gt;In this section, you'll configure Red Hat's SSO to use Kerberos.&lt;/p&gt; &lt;h3&gt;Configure the service principal and keytab file&lt;/h3&gt; &lt;p&gt;In your Red Hat single sign-on console, navigate to &lt;strong&gt;Red Hat SSO Admin console &gt; Realm &gt; User Federation &gt; Add provider &gt; Kerberos &lt;/strong&gt; to configure the HTTP service principal and generated keytab (&lt;code&gt;jboss_s2_Example.keytab&lt;/code&gt;) from the service principal (Figure 2). The keytab file should be present on the host where the SSO server is running. For the current walkthrough, that host is &lt;code&gt;s2.example.com&lt;/code&gt;.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/settings.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/settings.png?itok=gaqDkiEb" width="1421" height="817" alt="Configure the service principal and keytab in the Required Settings screen." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Configure the service principal and keytab in the Required Settings screen. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Configure the service principal and keytab in the Required Settings screen.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Enable Kerberos processing&lt;/h3&gt; &lt;p&gt;Enable Kerberos in the &lt;strong&gt;Flow&lt;/strong&gt; tab of the &lt;strong&gt;Authentication&lt;/strong&gt; screen in the console of Red Hat's SSO (Figure 3). For our purposes, you can set the Kerberos requirement to either &lt;strong&gt;ALTERNATIVE&lt;/strong&gt; or &lt;strong&gt;REQUIRED&lt;/strong&gt;. (&lt;strong&gt;ALTERNATIVE&lt;/strong&gt; means Kerberos is optional and if the user system is not configured to work with SPNEGO and Kerberos, then Red Hat's SSO will fall back to its regular login screens. &lt;strong&gt;REQUIRED &lt;/strong&gt; does not allow the user to fall back if their system is not configured to work with SPNEGO and Kerberos.)&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/authent.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/authent.png?itok=IuWkrV6N" width="1440" height="484" alt="Kerberos is configured in the Authentication screen." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Kerberos is configured in the Authentication screen. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Configure Kerberos in the Authentication screen.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Set up your client machine&lt;/h2&gt; &lt;p&gt;The Kerberos client should be able to connect to the KDC on &lt;code&gt;s1.example.com&lt;/code&gt; and exchange tickets. The client can procure service tickets from the KDC. A service ticket will be used to authenticate with the SSO server, making use of the Kerberos user federation created in the previous step.&lt;/p&gt; &lt;h3&gt;Configure the Kerberos client&lt;/h3&gt; &lt;p&gt;To allow users to communicate and authenticate with the Kerberos server, &lt;code&gt;krb5.conf&lt;/code&gt; must be configured on the user machine. This configuration file should point to the appropriate &lt;code&gt;kdc&lt;/code&gt; and &lt;code&gt;admin_server&lt;/code&gt;—remember, in the current example these are both &lt;code&gt;s1.example.com&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;[domain_realm]&lt;/code&gt; section provides a translation from a domain name or hostname to a Kerberos realm name. For the current example, the &lt;code&gt;.example.com&lt;/code&gt; domain maps to the EXAMPLE.COM realm.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;includedir /etc/krb5.conf.d/ [logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log [libdefaults] dns_lookup_realm = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true rdns = false pkinit_anchors = FILE:/etc/pki/tls/certs/ca-bundle.crt spake_preauth_groups = edwards25519 default_realm = EXAMPLE.COM default_ccache_name = KEYRING:persistent:%{uid} [realms] EXAMPLE.COM = { kdc = s1.example.com admin_server = s1.example.com } [domain_realm] .example.com = EXAMPLE.COM example.com = EXAMPLE.COM &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Enable logins through SPNEGO&lt;/h3&gt; &lt;p&gt;Next, you need to enable SPNEGO logins in the browser. To enable them in Mozilla Firefox, allow &lt;code&gt;.example.com&lt;/code&gt; (note the presence of the initial period) in the &lt;code&gt;network.negotiate-auth.trusted-uris&lt;/code&gt; configuration option as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;In the address bar of Firefox, type &lt;strong&gt;about:config&lt;/strong&gt; to display the list of current configuration options.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;In the &lt;strong&gt;Filter&lt;/strong&gt; field, type &lt;strong&gt;&lt;code&gt;negotiate&lt;/code&gt;&lt;/strong&gt; to restrict the list of options.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Double-click the &lt;strong&gt;network.negotiate-auth.trusted-uris&lt;/strong&gt; entry.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Enter the name of the domain against which to authenticate, including the preceding period (&lt;strong&gt;.&lt;/strong&gt;). If you want to add multiple domains, enter them in a comma-separated list. The result should look like Figure 4.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig4_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig4_2.png?itok=_iy-3tEa" width="600" height="79" alt="Screenshot showing how to configure Mozilla to use Kerberos" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Configuring Mozilla to use Kerbereos. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Authenticate to Red Hat's SSO account console&lt;/h2&gt; &lt;p&gt;Now that your systems are set up to use Red Hat's SSO and Kerberos, a user can initiate authentication by entering Kerberos's &lt;code&gt;kinit&lt;/code&gt; command, as in the following listing. The rest of this section lays out the sequence of events, as diagrammed in Figure 1, that the servers go through to satisfy the user's request. Figure 5 illustrates the Kerberos Authentication Service request and response.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [rissingh@s2 configuration]$ klist klist: Credentials cache 'KCM:109626' not found [rissingh@s2 configuration]$ env KRB5_CONFIG=/home/rissingh/Desktop/Rough/Cases/RHSSO_Rough/Cases/Raw_Setup/kerberos/krb5.conf kinit rishabh Password for rishabh@EXAMPLE.COM: [rissingh@s2 configuration]$ klist Ticket cache: KCM:109626 Default principal: rishabh@EXAMPLE.COM Valid starting Expires Service principal 08/27/2021 23:23:24 08/28/2021 23:23:24 krbtgt/EXAMPLE.COM@EXAMPLE.COM renew until 08/27/2021 23:23:24 &lt;/code&gt; &lt;/pre&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig5_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig5_2.png?itok=SK1AN0Zb" width="600" height="31" alt="Screenshot" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Kerberos Authentication Service request and response.&lt;/figcaption&gt; &lt;/figure&gt; &lt;ol&gt; &lt;li&gt; &lt;p&gt;The initial client request is an AS-REQ message, which asks for a TGT from the KDC. (Figure 6 and the subsequent figures are screenshots of the &lt;code&gt;tcpdump&lt;/code&gt; command offering visibility into the processing happening behind the scenes.)&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig6_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig6_2.png?itok=MRu0yOLs" width="460" height="321" alt="tcpdump screenshot" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Requesting a TGT.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The AS processes the AS-REQ and responds back with an AS-REP message that includes the TGT.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig7_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig7_2.png?itok=uQqFY6DP" width="374" height="295" alt="tcpdump screenshot" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Responding with a TGT.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The user now has access to account management on Red Hat's SSO, which, assuming you've followed the steps outlined in this article, is now a Kerberized service.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig8.png?itok=SKze1OYL" width="600" height="338" alt="Screenshot showing Kerberos authentication flow" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: Red Hat SSO is now a Kerberized service.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A TGS-REQ message, initiated to the Ticket Granting Service (TGS) for a service ticket, includes the TGT received in the AS-REP message earlier.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig9_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig9_0.png?itok=bTgXNOPQ" width="540" height="263" alt="tcpdump screenshot" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: A TGS-REQ message.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The Ticket Granting Service responds to the TGS-REQ request with a TGS-REP message that includes the requested service ticket.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig10_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig10_0.png?itok=vg-WDtkW" width="462" height="293" alt="tcpdump screenshot" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: A TGS-REP message.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Red Hat's SSO responds back with an HTTP 401 message containing a &lt;code&gt;WWW-Authenticate: Negotiate&lt;/code&gt; header.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig11.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig11.png?itok=F-VLcpgm" width="600" height="338" alt="Kerbereos authentication flow screenshot" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 11: WWW-Authenticate: Negotiate header received from Red Hat's SSO.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Execute the &lt;code&gt;klist&lt;/code&gt; command to list the service ticket received for the &lt;code&gt;HTTP/s2.example.com@EXAMPLE.COM&lt;/code&gt; service principal in step 5.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [rissingh@s2 configuration]$ klist Ticket cache: KCM:109626 Default principal: rishabh@EXAMPLE.COM Valid starting Expires Service principal 08/27/2021 23:23:24 08/28/2021 23:23:24 krbtgt/EXAMPLE.COM@EXAMPLE.COM renew until 08/27/2021 23:23:24 08/27/2021 23:24:40 08/28/2021 23:23:24 HTTP/s2.example.com@EXAMPLE.COM renew until 08/27/2021 23:23:24 [rissingh@s2 configuration]$ &lt;/code&gt; &lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;If the browser has the service ticket, it then transfers the SPNEGO token to Red Hat's SSO in the &lt;code&gt;Authorization: Negotiate 'spnego-token'&lt;/code&gt; header.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig12.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig12.png?itok=7yuLU54U" width="600" height="338" alt="Screenshot of Kerbereos authentication flow" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 12: &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 12: Transfering the SPNEGO token to Red Hat's SSO.&lt;/figcaption&gt; &lt;/figure&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Red Hat's SSO authenticates the user using the SPNEGO token.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;p&gt;When all these steps finish, the &lt;strong&gt;Account&lt;/strong&gt; screen for Red Hat's SSO is displayed in the browser, as shown in Figures 13 and 14.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig13.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig13.png?itok=UH3GX3Hj" width="600" height="254" alt="Screenshot of an account screen of an authenticated user" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 13: Account screen of an authenticated user. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig14.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig14.png?itok=kZGDxv8n" width="600" height="288" alt="Screenshot showing user information added in Red Hat's SSO." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 14: User information added in Red Hat's SSO. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Kerberos has been a fixture of federated security for decades. Red Hat's single sign-on technology, which enables you to secure your web applications, works well with Kerberos to provide a single sign-on experience.&lt;/p&gt; &lt;p&gt;Read more about Red Hat's SSO on Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/21/add-security-quarkus-application-using-red-hats-sso"&gt;Add security to a Quarkus application using Red Hat's SSO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/20/deploy-keycloak-single-sign-ansible"&gt;Deploy Keycloak single sign-on with Ansible&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/28/use-red-hats-single-sign-technology-secure-services-through-kerberos" title="Use Red Hat's single sign-on technology to secure services through Kerberos"&gt;Use Red Hat's single sign-on technology to secure services through Kerberos&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Rishabh Singh</dc:creator><dc:date>2022-04-28T07:00:00Z</dc:date></entry><entry><title type="html">Getting started with ZGC Garbage collector</title><link rel="alternate" href="http://www.mastertheboss.com/java/getting-started-with-zgc-garbage-collector/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/getting-started-with-zgc-garbage-collector/</id><updated>2022-04-27T15:33:10Z</updated><content type="html">This article will discuss the Java ZGC garbage collector and how you can configure and use it to meet your needs in modern Java applications. What is the Java ZGC? The Java ZGC is a scalable low latency garbage collector that is production-ready since Java 15. The ZGC Collector is designed to minimize GC pauses, ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Create a PrivateLink Red Hat OpenShift cluster on AWS with STS</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts" /><author><name>Suresh Gaikwad</name></author><id>ebc4b271-94a2-4852-bcca-4f3dbbf7c781</id><updated>2022-04-27T07:00:00Z</updated><published>2022-04-27T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cloud.redhat.com/products/amazon-openshift"&gt;Red Hat OpenShift Service on AWS&lt;/a&gt; is a version of the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; hosting service managed by Amazon Web Services (AWS). Although your cluster's own integrity is secure in that environment, communicating safely outside the cluster requires considerable setup. In this article, you'll learn how to connect securely through a firewall to the internet while keeping your cluster in a private workspace. We use Amazon's &lt;a href="https://aws.amazon.com/vpc/"&gt;Virtual Private Cloud &lt;/a&gt;(VPC), &lt;a href="https://wa.aws.amazon.com/wat.concept.sts.en.html"&gt;Security Token Service&lt;/a&gt; (STS), and &lt;a href="https://aws.amazon.com/transit-gateway/"&gt;AWS Transit Gateway&lt;/a&gt; to effect secure connections.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: To follow along with this article, you should be familiar with the AWS command-line interface (CLI), and have a basic understanding of AWS networking, routing, AWS permissions, transit gateways, and OpenShift, as well as &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; shell commands.&lt;/p&gt; &lt;h2&gt;Secure connections to managed cloud services&lt;/h2&gt; &lt;p&gt;There has been a recent industry-wide shift to managed services, and Red Hat has begun to see users migrate from the self-managed OpenShift Container Platform to the OpenShift Service on AWS. Such a move allows you to take advantage of a managed OpenShift cluster while focusing business resources where they are most needed.&lt;/p&gt; &lt;p&gt;During these migrations, there is often discussion among application platform, infrastructure, cloud, networking, and security teams around the specific resources created during provisioning and these would fit into any existing architectures users may have. The solution outlined in this article will help you understand how STS can help to enhance your security, and how you can manage all your outbound internet communication securely from one place without compromising VPC isolation. By consolidating your outbound traffic, you can manage outbound communications security, scaling, and configuration in one place.&lt;/p&gt; &lt;p&gt;OpenShift Service on AWS private clusters with &lt;a href="https://aws.amazon.com/privatelink/?privatelink-blogs.sort-by=item.additionalFields.createdDate&amp;privatelink-blogs.sort-order=desc"&gt;AWS PrivateLink&lt;/a&gt; are completely private. Red Hat site reliability engineering teams will make use of PrivateLink endpoints to access the cluster for management. You don't need public subnets, route tables, or an internet gateway. Typically, OpenShift Service on AWS private clusters with PrivateLink uses a transit gateway where the VPC for OpenShift Service on AWS will not have internet access. Traffic will flow from the OpenShift Service on AWS VPC to either an on-premise system or to another VPC or AWS account that provides a single controlled point of egress.&lt;/p&gt; &lt;p&gt;The scenario we'll describe in this article uses two VPCs: A private VPC in OpenShift Service on AWS and a public-facing VPC called the egress VPC. The private VPC contains only a single, private subnet, where all of the cluster resources reside. The egress VPC has a private subnet that communicates with the private VPC through the AWS Transit Gateway, and a public subnet that filters internet traffic through a standard firewall using network address translation (NAT).&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Although the example uses a single subnet in OpenShift Service on AWS for simplicity, we strongly recommend that a production cluster use multiple availability zones to minimize the potential for outages.&lt;/p&gt; &lt;p&gt;Figure 1 shows the overall architecture. Figure 2 shows how the egress VPC handles traffic to and from the internet. Over the course of this article, you'll see the commands that set up all these resources.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_3.png?itok=ypOG6g5k" width="1440" height="796" alt="Our architecture connects an egress VPC for public access with a private VPC." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Our architecture connects an egress VPC for public access with a private VPC. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The example architecture connects an egress VPC for public access with a private VPC.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/egress.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/egress.png?itok=-4M7z91D" width="1440" height="819" alt="The egress VPC runs traffic to and from the Internet through a NAT gateway." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The egress VPC runs traffic to and from the Internet through a NAT gateway. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The egress VPC runs traffic to and from the internet through a NAT gateway&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;The procedure in this article requires:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html"&gt;AWS CLI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The OpenShift Service on AWS &lt;a href="https://github.com/openshift/rosa/releases/tag/v1.1.7"&gt;CLI&lt;/a&gt; (&lt;code&gt;rosa&lt;/code&gt;), version 1.1.7&lt;/li&gt; &lt;li&gt;The &lt;a href="https://stedolan.github.io/jq/download/"&gt;jq&lt;/a&gt; command-line JSON processor&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Before you create an OpenShift Service on AWS cluster that uses STS, you must complete the AWS prerequisites, verify that the required &lt;a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-required-aws-service-quotas.html"&gt;AWS service quotas&lt;/a&gt; are available, and set up your environment.&lt;/p&gt; &lt;p&gt;Please follow the &lt;a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-aws-prereqs.html"&gt;OpenShift Service on AWS documentation&lt;/a&gt; to set up the account prerequisites. Review the identity and access management (IAM) policies, STS version, and firewall and security group prerequisites. Then &lt;a href="https://docs.openshift.com/rosa/rosa_getting_started/rosa-config-aws-account.html"&gt;configure&lt;/a&gt; your AWS account and enable OpenShift Service on AWS.&lt;/p&gt; &lt;h2 id="prerequisites"&gt;Create your private VPC&lt;/h2&gt; &lt;ul&gt; &lt;/ul&gt; &lt;p&gt;If this is a brand new AWS account that has never had an AWS Load Balancer installed, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws iam create-service-linked-role --aws-service-name "elasticloadbalancing.amazonaws.com"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Configure the following environment variables, submitting your own values for the &lt;code&gt;ROSA_CLUSTER_NAME&lt;/code&gt;, &lt;code&gt;VERSION&lt;/code&gt;, and &lt;code&gt;REGION&lt;/code&gt; environment variables.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export VERSION=4.9.21 ROSA_CLUSTER_NAME=suresh-rosa AWS_DEFAULT_REGION=ap-southeast-1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the private VPC where the OpenShift Service on AWS cluster will be installed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ VPC_ID_1=`aws ec2 create-vpc --cidr-block 10.1.0.0/16 | jq -r .Vpc.VpcId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add a tag for the OpenShift Service on AWS private VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $VPC_ID_1 --tags Key=Name,Value=rosa_intranet_vpc&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create your egress VPC&lt;/h2&gt; &lt;p&gt;Create the egress VPC with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ VPC_ID_2=`aws ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r .Vpc.VpcId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tag the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $VPC_ID_2 --tags Key=Name,Value=egress_vpc&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Set up DNS&lt;/h2&gt; &lt;p&gt;Configure the VPCs to allow DNS hostnames for their public IP addresses:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 modify-vpc-attribute --vpc-id $VPC_ID_1 --enable-dns-hostnames $ aws ec2 modify-vpc-attribute --vpc-id $VPC_ID_2 --enable-dns-hostnames&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the subnets&lt;/h2&gt; &lt;p&gt;Create a private subnet in the OpenShift Service on AWS private VPC where cluster instances will be running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ROSA_PRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID_1 --cidr-block 10.1.0.0/17 | jq -r .Subnet.SubnetId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tag the private subnet in the OpenShift Service on AWS private VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $ROSA_PRIVATE_SUBNET --tags Key=Name,Value=intranet-pvt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a private subnet in the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ EGRESS_PRIVATE_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID_2 --cidr-block 10.0.0.0/17 | jq -r .Subnet.SubnetId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tag the private subnet in the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $EGRESS_PRIVATE_SUBNET --tags Key=Name,Value=egress-pvt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a public subnet in the egress VPC to egress the traffic to the internet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ EGRESS_PUBLIC_SUBNET=`aws ec2 create-subnet --vpc-id $VPC_ID_2 --cidr-block 10.0.128.0/17 | jq -r .Subnet.SubnetId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tag the public subnet in the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $EGRESS_PUBLIC_SUBNET --tags Key=Name,Value=egress-public&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the internet gateway in the egress VPC&lt;/h2&gt; &lt;p&gt;Create the internet gateway with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ I_GW=`aws ec2 create-internet-gateway | jq -r .internetGateway.internetGatewayId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tag the internet gateway:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $I_GW --tags Key=Name,Value=suresh_rosa_cluster&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Attach the internet gateway to the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 attach-internet-gateway --vpc-id $VPC_ID_2 --internet-gateway-id $I_GW&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the NAT gateway in the egress VPC&lt;/h2&gt; &lt;p&gt;Allocate an Elastic IP address:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ EIP=`aws ec2 allocate-address --domain vpc | jq -r .AllocationId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the NAT gateway with the following command and allocate the new Elastic IP address:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ NAT_GATEWAY=`aws ec2 create-nat-gateway --subnet-id $EGRESS_PUBLIC_SUBNET --allocation-id $EIP | jq -r .NatGateway.NatGatewayId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tag the Elastic IP address:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $EIP --resources $NAT_GATEWAY --tags Key=Name,Value=egress_nat_public&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new NAT gateway should now be created and associated with your VPC.&lt;/p&gt; &lt;h2&gt;Create the AWS transit gateway&lt;/h2&gt; &lt;p&gt;Create a transit gateway to attach the two VPCs as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ T_GW=`aws ec2 create-transit-gateway | jq -r .TransitGateway.TransitGatewayId` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Tag the transit gateway:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $T_GW --tags Key=Name,Value=suresh-transit-gateway&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The transit gateway starts in the pending state and will move to an available state in a few minutes. Once the transit gateway is in the available state, create a transit gateway VPC attachment for the OpenShift Service on AWS private VPC with a private subnet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ T_GW_A_RPV=`aws ec2 create-transit-gateway-vpc-attachment --transit-gateway-id $T_GW --vpc-id $VPC_ID_1 --subnet-ids $ROSA_PRIVATE_SUBNET | jq -r .TransitGatewayVpcAttachment.TransitGatewayAttachmentId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add a tag for the transit gateway attachment for the OpenShift Service on AWS private VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $T_GW_A_RPV --tags Key=Name,Value=transit-gw-intranet-attachment&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the transit gateway VPC attachment for the egress VPC with a private subnet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ T_GW_A_EPV=`aws ec2 create-transit-gateway-vpc-attachment --transit-gateway-id $T_GW --vpc-id $VPC_ID_2 --subnet-ids $EGRESS_PRIVATE_SUBNET | jq -r .TransitGatewayVpcAttachment.TransitGatewayAttachmentId` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add a tag for the transit gateway attachment for the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $T_GW_A_EPV --tags Key=Name,Value=transit-gw-egress-attachment&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Egress gateway route&lt;/h2&gt; &lt;p&gt;Grab the default transit gateway's route table ID:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ T_GW_D_RT=`aws ec2 describe-transit-gateways --transit-gateway-id $T_GW | jq -r '.TransitGateways | .[] | .Options.AssociationDefaultRouteTableId'`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add a tag for the transit gateway's route table:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $T_GW_D_RT --tags Key=Name,Value=transit-gw-rt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add a static route for internet traffic to go to the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-transit-gateway-route --destination-cidr-block 0.0.0.0/0 --transit-gateway-route-table-id $T_GW_D_RT --transit-gateway-attachment-id $T_GW_A_EPV&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Grab the main route table associated with the OpenShift Service on AWS private VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ROSA_VPC_MAIN_RT=`aws ec2 describe-route-tables --filters 'Name=vpc-id,Values='$VPC_ID_1'' --query 'RouteTables[].Associations[].RouteTableId' | jq .[] | tr -d '"'`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add a tag for the OpenShift Service on AWS VPC's main route table:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-tags --resources $ROSA_VPC_MAIN_RT --tags Key=Name,Value=rosa_main_rt &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Grab the main route table associated with the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ EGRESS_VPC_MAIN_RT=`aws ec2 describe-route-tables --filters 'Name=vpc-id,Values='$VPC_ID_2'' --query 'RouteTables[].Associations[].RouteTableId' | jq .[] | tr -d '"'`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a private route table in the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ EGRESS_PRI_RT=`aws ec2 create-route-table --vpc-id $VPC_ID_2 | jq -r .RouteTable.RouteTableId`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Associate the private subnet from the egress VPC:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 associate-route-table --route-table-id $EGRESS_PRI_RT --subnet-id $EGRESS_PRIVATE_SUBNET&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;NAT gateway route&lt;/h2&gt; &lt;p&gt;Create a route in the egress private route table for all addresses to the NAT gateway:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-route --route-table-id $EGRESS_PRI_RT --destination-cidr-block 0.0.0.0/0 --gateway-id $NAT_GATEWAY &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a route in the egress VPC's main route table for all addresses going to the internet gateway:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-route --route-table-id $EGRESS_VPC_MAIN_RT --destination-cidr-block 0.0.0.0/0 --gateway-id $I_GW &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a route in the egress VPC's main route table to direct addresses in the OpenShift Service on AWS private VPC to the transit gateway:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-route --route-table-id $EGRESS_VPC_MAIN_RT --destination-cidr-block 10.1.0.0/16 --gateway-id $T_GW &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a route in the OpenShift Service on AWS private route table to direct all of its addresses to the transit gateway:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 create-route --route-table-id $ROSA_VPC_MAIN_RT --destination-cidr-block 0.0.0.0/0 --gateway-id $T_GW &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create a cluster using Red Hat OpenShift Service on AWS&lt;/h2&gt; &lt;p&gt;Make sure that the &lt;code&gt;rosa&lt;/code&gt; binary is downloaded and available in the current working directory with executable permissions set. Then enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./rosa create account-roles --mode auto --yes&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create your cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./rosa create cluster -y --cluster-name $ROSA_CLUSTER_NAME --region $AWS_DEFAULT_REGION --version $VERSION --private-link --machine-cidr=10.1.0.0/16 --sts --subnet-ids=$ROSA_PRIVATE_SUBNET&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output of this command should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;I: Using arn:aws:iam::XXXXX:role/ManagedOpenShift-Installer-Role for the Installer role I: Using arn:aws:iam::XXXXX:role/ManagedOpenShift-ControlPlane-Role for the ControlPlane role I: Using arn:aws:iam::XXXXX:role/ManagedOpenShift-Worker-Role for the Worker role I: Using arn:aws:iam::XXXXX:role/ManagedOpenShift-Support-Role for the Support role W: You are choosing to use AWS PrivateLink for your cluster. STS clusters can only be private if AWS PrivateLink is used. Once the cluster is created, this option cannot be changed. I: Creating cluster 'suresh-rosa' I: To view a list of clusters and their status, run 'rosa list clusters' I: Cluster 'suresh-rosa' has been created. I: Once the cluster is installed you will need to add an Identity Provider before you can login into the cluster. See 'rosa create idp --help' for more information. I: To determine when your cluster is Ready, run 'rosa describe cluster -c suresh-rosa'. I: To watch your cluster installation logs, run 'rosa logs install -c suresh-rosa --watch'. Name: suresh-rosa ID: 1qi0bhb8o7fighppuft1n6cm5e8k2p36 External ID: OpenShift Version: Channel Group: stable DNS: suresh-rosa.sv9i.p1.openshiftapps.com AWS Account: XXXXX API URL: Console URL: Region: ap-southeast-1 Multi-AZ: false Nodes: - Control plane: 3 - Infra: 2 - Compute: 2 Network: - Service CIDR: 172.30.0.0/16 - Machine CIDR: 10.0.0.0/16 - Pod CIDR: 10.128.0.0/14 - Host Prefix: /23 STS Role ARN: arn:aws:iam::XXXXX:role/ManagedOpenShift-Installer-Role Support Role ARN: arn:aws:iam::XXXXX:role/ManagedOpenShift-Support-Role Instance IAM Roles: - Control plane: arn:aws:iam::XXXXX:role/ManagedOpenShift-ControlPlane-Role - Worker: arn:aws:iam::XXXXX:role/ManagedOpenShift-Worker-Role Operator IAM Roles: - arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-machine-api-aws-cloud-credentials - arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-cloud-credential-operator-cloud-crede - arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-image-registry-installer-cloud-creden - arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-ingress-operator-cloud-credentials - arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-cluster-csi-drivers-ebs-cloud-credent State: waiting (Waiting for OIDC configuration) Private: Yes Created: Mar 01 2022 15:33:27 UTC Details Page: https://console.redhat.com/openshift/details/s/25W6HXZWTTdk35T4ERcFTRZHHIb OIDC Endpoint URL: https://XX-oidc.s3.us-east-1.amazonaws.com/1qi0bhb8o7fighppuft1n6cm5e8k2p3 I: Run the following commands to continue the cluster creation: rosa create operator-roles --cluster suresh-rosa rosa create oidc-provider --cluster suresh-rosa &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the Operator provider:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./rosa create operator-roles --cluster $ROSA_CLUSTER_NAME --mode auto -y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output of this command should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;? Permissions boundary ARN (optional): ? Role creation mode: auto I: Creating roles using 'arn:aws:iam::XXXXX:user/user' I: Created role 'suresh-rosa-f0l3-openshift-cluster-csi-drivers-ebs-cloud-credent' with ARN 'arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-cluster-csi-drivers-ebs-cloud-credent' I: Created role 'suresh-rosa-f0l3-openshift-machine-api-aws-cloud-credentials' with ARN 'arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-machine-api-aws-cloud-credentials' I: Created role 'suresh-rosa-f0l3-openshift-cloud-credential-operator-cloud-crede' with ARN 'arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-cloud-credential-operator-cloud-crede' I: Created role 'suresh-rosa-f0l3-openshift-image-registry-installer-cloud-creden' with ARN 'arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-image-registry-installer-cloud-creden' I: Created role 'suresh-rosa-f0l3-openshift-ingress-operator-cloud-credentials' with ARN 'arn:aws:iam::XXXXX:role/suresh-rosa-f0l3-openshift-ingress-operator-cloud-credentials'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create the OpenID Connect (OIDC) provider:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./rosa create oidc-provider --cluster $ROSA_CLUSTER_NAME --mode auto -y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output of this command should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;? OIDC provider creation mode: auto I: Creating OIDC provider using 'arn:aws:iam::XXXXX:user/user' I: Created OIDC provider with ARN 'arn:aws:iam::XXXXX:oidc-provider/XX-oidc.s3.us-east-1.amazonaws.com/1qhvtf5n3n4pvnjmeqe37dj6fnsq0htm'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Other administrative tasks&lt;/h2&gt; &lt;p&gt;Watch the installation logs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./rosa logs install -c $ROSA_CLUSTER_NAME --watch&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an OpenShift Service on AWS administrative user and save the login command for later use:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./rosa create admin -c $ROSA_CLUSTER_NAME&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output of this command should look like the following. Copy and save the &lt;code&gt;oc login&lt;/code&gt; command, which can be used to log in to the cluster from the CLI:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;W: It is recommended to add an identity provider to login to this cluster. See 'rosa create idp --help' for more information. I: Admin account has been added to cluster 'suresh-rosa'. I: Please securely store this generated password. If you lose this password you can delete and recreate the cluster admin user. I: To login, run the following command: oc login https://api.suresh-rosa.sv9i.p1.openshiftapps.com:6443 --username cluster-admin --password KfVVi-GgtcP-uSgYc-2c9u3 I: It may take up to a minute for the account to become active.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;List the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ./rosa list cluster&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the cluster has finished installing, it's time to validate it. Validation when using a private link requires the use of a jump host. Create a jump host in the public subnet with the following command, replacing &lt;code&gt;&lt;ami-id&gt;&lt;/code&gt; with the exact ami-id from your environment and &lt;code&gt;rosakeypair&lt;/code&gt; with the SSH key pair details:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws ec2 run-instances --image-id &lt;ami-id&gt; --count 1 --instance-type t2.micro --key-name rosakeypair --subnet-id $EGRESS_PUBLIC_SUBNET --associate-public-ip-address&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Refer to AWS documentation for more options you can use to create the instances.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article was inspired by real-world customer experience. If you want to traverse the traffic from multiple VPCs before network packets goe outside your infrastructure from OpenShift on AWS cluster, or when network packets enter your infrastructure to communicate with OpenShift on AWS cluster, the approach taken in this article will help you to achieve your requirements. You can use the same approach to connect more than two VPCs to forward the traffic from the internal VPC to the internet. In this way, after completing the steps outlined here, your private resources can communicate with internet.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts" title="Create a PrivateLink Red Hat OpenShift cluster on AWS with STS"&gt;Create a PrivateLink Red Hat OpenShift cluster on AWS with STS&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Suresh Gaikwad</dc:creator><dc:date>2022-04-27T07:00:00Z</dc:date></entry><entry><title>Monitoring Quarkus JVM Mode With Cryostat</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/monitoring-quarkus-jvm-mode-with-cryostat/&#xA;            " /><author><name>Andrew Azores</name></author><id>https://quarkus.io/blog/monitoring-quarkus-jvm-mode-with-cryostat/</id><updated>2022-04-27T00:00:00Z</updated><published>2022-04-27T00:00:00Z</published><summary type="html">Cryostat is a profiling and monitoring tool that leverages the JDK Flight Recorder (JFR) framework already present in your Java applications running on the HotSpot JVM. Cryostat provides an in-cluster collection hub for easy and secure access to your JDK Flight Recorder data from outside the cluster. Cryostat is a...</summary><dc:creator>Andrew Azores</dc:creator><dc:date>2022-04-27T00:00:00Z</dc:date></entry><entry><title type="html">RESTEasy 6.1.0.Beta2 Release</title><link rel="alternate" href="https://resteasy.github.io/2022/04/26/resteasy-6.1.0.Beta2/" /><author><name /></author><id>https://resteasy.github.io/2022/04/26/resteasy-6.1.0.Beta2/</id><updated>2022-04-26T18:11:11Z</updated><dc:creator /></entry><entry><title>Orchestrate offloaded network functions on DPUs with Red Hat OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/26/orchestrate-offloaded-network-functions-dpus-red-hat-openshift" /><author><name>Erwan Gallen</name></author><id>bac918cf-fe82-4689-a022-2dc4156d304a</id><updated>2022-04-26T10:00:00Z</updated><published>2022-04-26T10:00:00Z</published><summary type="html">&lt;p&gt;The traditional CPU-centric system architecture is being replaced by designs where systems are aggregated from independently intelligent devices. These systems have their own compute capabilities and can natively run network functions with an accelerated data plane. The new model allows us to offload to accelerators not only the individual subroutines but whole software subsystems, such as networking or storage, with cloud-like security isolation and architectural compartmentalization.&lt;/p&gt; &lt;p&gt;One of the most prominent examples of this new architecture is the data processing unit (DPU). DPUs offer a complete compute system with an independent software stack, network identity, and provisioning capabilities. The DPU can host its own applications using either embedded or orchestrated deployment models.&lt;/p&gt; &lt;p&gt;The unique capabilities of the DPU allow for key infrastructure functions and their associated software stacks to be completely removed from the host node’s CPU cores and to be relocated onto the DPU. For instance, DPU could host the management plane of the network functions and part of the control plane, while the data plane could be accelerated by dedicated Arm cores, ASICs, GPUs, or FPGA IPs. Because DPUs can run independent software stacks locally, multiple network functions could run simultaneously on the same devices with service chaining and shared accelerators to provide generic in-line processing.&lt;/p&gt; &lt;h2&gt;OVN/OVS offloading on NVIDIA BlueField-2 DPUs&lt;/h2&gt; &lt;p&gt;Red Hat has collaborated with NVIDIA to extend the operational simplicity and hybrid cloud architecture of &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; to NVIDIA Bluefield-2 DPUs. Red Hat OpenShift 4.10 provides BlueField-2 OVN/OVS offload as a developer preview.&lt;/p&gt; &lt;p&gt;Installing OpenShift Container Platform on a DPU makes it possible to offload packet processing from the host CPUs to the DPU. Offloading resource-intensive tasks like packet processing from the server’s CPU to the DPU can free up cycles on the OpenShift worker nodes to run more user applications. OpenShift brings portability, scalability, and orchestration to DPU workloads, giving you the ability to use standard &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; APIs along with consistent system management interfaces for both the host systems and DPUs.  &lt;/p&gt; &lt;p&gt;In short, utilizing OpenShift with DPUs lets you get the benefits of DPUs without sacrificing the hybrid cloud experience or adding unnecessary complexity to managing IT infrastructure.&lt;/p&gt; &lt;p&gt;To manage DPUs, Red Hat OpenShift replaces the native BlueField operating system (OS) on each DPU and is deployed using a two-cluster design that consists of:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;Tenant cluster running on the host servers (x86)&lt;/li&gt; &lt;li aria-level="1"&gt;Infrastructure cluster running on DPUs (Arm)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The architecture is illustrated in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image5_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image5_0.png?itok=jfJF-JJg" width="600" height="262" alt="Diagram of the architecture." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The two-cluster design.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;In this architecture, DPUs are provisioned as worker nodes of the Arm-based OpenShift infrastructure cluster. This is the blue cluster in Figure 1. The tenant OpenShift cluster, composed of the x86 servers, is where user applications typically run. This is the green cluster. In this deployment, each physical server runs both a tenant node on the x86 cores and an infrastructure node on the DPU Arm cores.&lt;/p&gt; &lt;p&gt;This architecture allows you to minimize the attack surface by decoupling the workload from the management cluster.&lt;/p&gt; &lt;p&gt;This architecture also streamlines operations by decoupling the application workload from the underlying infrastructure. That allows IT Ops to deploy and maintain the platform software and accelerated infrastructure while DevOps deploys and maintains application workloads independently from the infrastructure layer.&lt;/p&gt; &lt;p&gt;Red Hat OpenShift 4.10 provides capabilities for offloading Open Virtual Network (&lt;a href="https://www.ovn.org/en/"&gt;&lt;u&gt;OVN&lt;/u&gt;&lt;/a&gt;) and Open Virtual Switch (&lt;a href="https://www.openvswitch.org/"&gt;&lt;u&gt;OVS&lt;/u&gt;&lt;/a&gt;) services that typically run on servers, from the host CPU to the DPU. We are offering this functionality as a developer preview and enabling the following components to support OVN/OVS hardware offload to NVIDIA BlueField-2 DPUs:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;DPU Network Operator: This component is used with the infrastructure cluster to facilitate OVN deployment.&lt;/li&gt; &lt;li aria-level="1"&gt;DPU mode for OVN Kubernetes: This component is assigned by the cluster network operator for the tenant cluster.&lt;/li&gt; &lt;li aria-level="1"&gt;SR-IOV network operator: This component discovers compatible network devices, such as the ConnectX-6 Dx NIC embedded inside the BlueField-2 DPU, and provisions them for SR-IOV access by pods on that server&lt;/li&gt; &lt;li aria-level="1"&gt;ConnectX NIC Fast Data Path on Arm&lt;/li&gt; &lt;li aria-level="1"&gt;Kernel flow offloading (TC Flower)&lt;/li&gt; &lt;li aria-level="1"&gt;Experimental use of OpenShift Assisted Installer and BlueField-2 BMC&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The combination of these components allows us to move ovn-kube-node services from the x86 host to the BlueField-2 DPU.&lt;/p&gt; &lt;p&gt;The network flows are offloaded in this manner (see Figure 2):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The ovn-k8s components are moved from the x86 host to the DPU (ovn-kube, vswitchd, ovsdb).&lt;/li&gt; &lt;li&gt;The Open vSwitch data path is offloaded from the BlueField Arm CPU to the ConnectX-6 Dx ASIC.&lt;/li&gt; &lt;/ul&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image4_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image4_1.png?itok=Vq8xo5CJ" width="600" height="450" alt="Diagram of the network flows." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Illustrating the network flows.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The following Open vSwitch datapath flows managed by ovn-k8s are offloaded to a BlueField-2 DPU that is running OpenShift 4.10:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pod to pod (east-west)&lt;/li&gt; &lt;li&gt;Pod to clusterIP service backed by a regular pod in diff node (east-west)&lt;/li&gt; &lt;li&gt;Pod to external (north-south)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let’s take a more detailed look at the testbed setup.&lt;/p&gt; &lt;h2&gt;Install and configure an accelerated infrastructure with OpenShift and NVIDIA Bluefield-2 DPUs&lt;/h2&gt; &lt;p&gt;In our sample testbed shown in Figure 3, we are using two x86 hosts with BlueField-2 DPU PCIe cards installed. Each DPU has eight Arm cores, two 25GB network ports, and a 1GbE management port. We’ve wired 25GB ports to a switch and also connected the BMC port of the DPU to a separate network to manage the device with IPMI.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image2.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image2.jpg?itok=imhaEiEG" width="600" height="450" alt="A photo of the sample testbed." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The sample testbed.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Next, we’ve installed Red Hat OpenShift Container Platform 4.10 on a DPU to offload packet processing from the host x86 to the DPU. Offloading resource-intensive computational tasks such as packet processing from the server’s CPU to the DPU frees up cycles on the OpenShift Container Platform worker nodes to run more applications or to run the same number of applications more quickly.&lt;/p&gt; &lt;h3&gt;OpenShift and DPU deployment architecture&lt;/h3&gt; &lt;p&gt;In our setup, OpenShift replaces the native Bluefield OS. We used the two-cluster architecture where DPU cards are provisioned as worker nodes in the Arm-based infrastructure cluster. The tenant cluster composed of x86 servers was used to run user applications.&lt;/p&gt; &lt;p&gt;We followed these steps to deploy tenant and infrastructure clusters:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install &lt;a href="https://cloud.redhat.com/blog/using-the-openshift-assisted-installer-service-to-deploy-an-openshift-cluster-on-metal-and-vsphere"&gt;&lt;u&gt;OpenShift assisted installer&lt;/u&gt;&lt;/a&gt; on the installer node using Podman.&lt;/li&gt; &lt;li&gt;Install the infrastructure cluster.&lt;/li&gt; &lt;li&gt;Install the tenant cluster.&lt;/li&gt; &lt;li&gt;Install the DPU Network Operator on the infrastructure cluster.&lt;/li&gt; &lt;li&gt;Configuring SR-IOV Operator for DPUs.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For details about how to install the OpenShift 4.10 on BlueField with OVN/OVS hardware offload, refer to the &lt;a href="https://access.redhat.com/articles/6804281"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;When the cluster is deployed on the BlueField, the OVN configuration is automated with the "DPU Network Operator"; this operator can be installed in the OpenShift console or with the &lt;code&gt;oc&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;As shown in Figure 4, the DPU operator is available for the Arm-based OpenShift clusters in the catalog and not visible in x86 OpenShift clusters.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image1_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image1_2.png?itok=zQH1Dn_1" width="600" height="509" alt="The DPU Network Operator shown in the Operator Hub catalog." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The DPU Network Operator shown in the OperatorHub catalog.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Validate installation using the OpenShift console&lt;/h2&gt; &lt;p&gt;When you have completed the last step, you have two OpenShift clusters running. We will make some console checks to show the configuration done and benchmark the OVS/OVN offloading.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tenant cluster:&lt;/strong&gt; We can list the nodes of the Tenant cluster nodes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-tenant ~]$ oc get nodes NAME STATUS ROLES AGE VERSION tenant-master-0 Ready master 47d v1.23.3+759c22b tenant-master-1 Ready master 47d v1.23.3+759c22b tenant-master-2 Ready master 47d v1.23.3+759c22b tenant-worker-0 Ready worker 47d v1.23.3+759c22b tenant-worker-1 Ready worker 47d v1.23.3+759c22b x86-worker-advnetlab13 Ready dpu-host,worker 35d v1.23.3+759c22b x86-worker-advnetlab14 Ready dpu-host,worker 41d v1.23.3+759c22b&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Infrastructure cluster:&lt;/strong&gt; We can list the nodes of the Infrastructure cluster, including the BlueField-2 nodes with the machine-config pool name &lt;code&gt;dpu-worker&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-infrastructure ~]$ oc get nodes NAME STATUS ROLES AGE VERSION bf-13 Ready dpu-worker,worker 71d v1.22.1+6859754 bf-14 Ready dpu-worker,worker 74d v1.22.1+6859754 master-0 Ready master 75d v1.22.1+6859754 master-1 Ready master 75d v1.22.1+6859754 master-2 Ready master 75d v1.22.1+6859754 worker-0 Ready worker 70d v1.22.1+6859754 worker-1 Ready worker 70d v1.22.1+6859754&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Tenant cluster:&lt;/strong&gt; We can list the pods running in the &lt;code&gt;openshift-ovn-kubernetes&lt;/code&gt; namespace in the tenant cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-tenant ~]$ oc get pods -n openshift-ovn-kubernetes NAME READY STATUS RESTARTS AGE ovnkube-master-99x8j 4/6 Running 7 25d ovnkube-master-qdfvv 6/6 Running 14 (15d ago) 25d ovnkube-master-w28mh 6/6 Running 7 (15d ago) 25d ovnkube-node-5xlxr 5/5 Running 5 34d ovnkube-node-6nkm5 5/5 Running 5 34d ovnkube-node-dpu-host-45crl 3/3 Running 50 34d ovnkube-node-dpu-host-r8tlj 3/3 Running 30 28d ovnkube-node-f2x2q 5/5 Running 0 34d ovnkube-node-j6w6t 5/5 Running 5 34d ovnkube-node-qtc6f 5/5 Running 0 34d [egallen@bastion-tenant ~]$ oc get pods -n openshift-ovn-kubernetes -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ovnkube-master-99x8j 4/6 Running 7 25d 192.168.111.122 tenant-master-2 &lt;none&gt; &lt;none&gt; ovnkube-master-qdfvv 6/6 Running 14 (15d ago) 25d 192.168.111.121 tenant-master-1 &lt;none&gt; &lt;none&gt; ovnkube-master-w28mh 6/6 Running 7 (15d ago) 25d 192.168.111.120 tenant-master-0 &lt;none&gt; &lt;none&gt; ovnkube-node-5xlxr 5/5 Running 5 34d 192.168.111.121 tenant-master-1 &lt;none&gt; &lt;none&gt; ovnkube-node-6nkm5 5/5 Running 5 34d 192.168.111.122 tenant-master-2 &lt;none&gt; &lt;none&gt; ovnkube-node-dpu-host-45crl 3/3 Running 50 34d 192.168.111.113 x86-worker-advnetlab13 &lt;none&gt; &lt;none&gt; ovnkube-node-dpu-host-r8tlj 3/3 Running 30 28d 192.168.111.114 x86-worker-advnetlab14 &lt;none&gt; &lt;none&gt; ovnkube-node-f2x2q 5/5 Running 0 34d 192.168.111.123 tenant-worker-0 &lt;none&gt; &lt;none&gt; ovnkube-node-j6w6t 5/5 Running 5 34d 192.168.111.120 tenant-master-0 &lt;none&gt; &lt;none&gt; ovnkube-node-qtc6f 5/5 Running 0 34d 192.168.111.124 tenant-worker-1 &lt;none&gt; &lt;none&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Infrastructure cluster: &lt;/strong&gt;The DaemonSet and the nodes have the same &lt;code&gt;dpu-worker&lt;/code&gt; label. The ovnkube-node host network pods will be scheduled on the BlueField-2:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-infrastructure ~]$ oc get ds -n default --show-labels NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE LABELS ovnkube-node 2 2 2 2 2 beta.kubernetes.io/os=linux,node-role.kubernetes.io/dpu-worker= 65d &lt;none&gt; [egallen@bastion-infrastructure ~]$ oc get nodes --show-labels | grep dpu-worker bf-13 Ready dpu-worker,worker 71d v1.22.1+6859754 beta.kubernetes.io/arch=arm64,beta.kubernetes.io/os=linux,kubernetes.io/arch=arm64,kubernetes.io/hostname=bf-13,kubernetes.io/os=linux,network.operator.openshift.io/dpu=,node-role.kubernetes.io/dpu-worker=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos bf-14 Ready dpu-worker,worker 74d v1.22.1+6859754 beta.kubernetes.io/arch=arm64,beta.kubernetes.io/os=linux,kubernetes.io/arch=arm64,kubernetes.io/hostname=bf-14,kubernetes.io/os=linux,network.operator.openshift.io/dpu=,node-role.kubernetes.io/dpu-worker=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Infrastructure cluster: &lt;/strong&gt;We see the ovnkube-node DaemonSet running on the two BlueField-2 infrastructure cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-infrastructure ~]$ oc get pods -n default NAME READY STATUS RESTARTS AGE ovnkube-node-hshxs 2/2 Running 13 (6d13h ago) 25d ovnkube-node-mng24 2/2 Running 17 (42h ago) 25d [egallen@bastion-infrastructure ~]$ oc get pods -n default -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ovnkube-node-hshxs 2/2 Running 13 (6d13h ago) 25d 192.168.111.28 bf-14 &lt;none&gt; &lt;none&gt; ovnkube-node-mng24 2/2 Running 17 (42h ago) 25d 192.168.111.29 bf-13 &lt;none&gt; &lt;none&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Infrastructure cluster:&lt;/strong&gt; We get the IP address of the BlueField bf-14 to &lt;code&gt;ssh&lt;/code&gt; on it (&lt;code&gt;192.168.111.28&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-infrastructure ~]$ oc get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME bf-13 Ready dpu-worker,worker 71d v1.22.1+6859754 192.168.111.29 &lt;none&gt; Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 (Ootpa) 4.18.0-305.40.1.el8_4.test.aarch64 cri-o://1.23.0-98.rhaos4.10.git9b7f5ae.el8 bf-14 Ready dpu-worker,worker 74d v1.22.1+6859754 192.168.111.28 &lt;none&gt; Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 (Ootpa) 4.18.0-305.40.1.el8_4.test.aarch64 cri-o://1.23.0-98.rhaos4.10.git9b7f5ae.el8 master-0 Ready master 75d v1.22.1+6859754 192.168.111.20 &lt;none&gt; Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 (Ootpa) 4.18.0-305.30.1.el8_4.aarch64 cri-o://1.23.0-98.rhaos4.10.git9b7f5ae.el8 master-1 Ready master 75d v1.22.1+6859754 192.168.111.21 &lt;none&gt; Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 (Ootpa) 4.18.0-305.30.1.el8_4.aarch64 cri-o://1.23.0-98.rhaos4.10.git9b7f5ae.el8 master-2 Ready master 75d v1.22.1+6859754 192.168.111.22 &lt;none&gt; Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 (Ootpa) 4.18.0-305.30.1.el8_4.aarch64 cri-o://1.23.0-98.rhaos4.10.git9b7f5ae.el8 worker-0 Ready worker 70d v1.22.1+6859754 192.168.111.23 &lt;none&gt; Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 (Ootpa) 4.18.0-305.30.1.el8_4.aarch64 cri-o://1.23.0-98.rhaos4.10.git9b7f5ae.el8 worker-1 Ready worker 70d v1.22.1+6859754 192.168.111.24 &lt;none&gt; Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 (Ootpa) 4.18.0-305.30.1.el8_4.aarch64 cri-o://1.23.0-98.rhaos4.10.git9b7f5ae.el8&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Infrastructure cluster:&lt;/strong&gt; We can &lt;code&gt;ssh&lt;/code&gt; to one BlueField-2 of the cluster to check the configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-infrastructure ~]$ ssh core@192.168.111.28 Red Hat Enterprise Linux CoreOS 410.84.202201071203-0 Part of OpenShift 4.10, RHCOS is a Kubernetes native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.10/architecture/architecture-rhcos.html --- Last login: Wed Mar 30 07:50:50 2022 from 192.168.111.1 [core@bf-14 ~]$ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On one BlueField: &lt;/strong&gt;We are running OpenShift 4.10 on the Arm cores:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[core@bf-14 ~]$ cat /etc/redhat-release Red Hat Enterprise Linux CoreOS release 4.10 [core@bf-14 ~]$ uname -m aarch64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On one BlueField: &lt;/strong&gt;The BlueField-2 has 8 x Armv8 A72 cores (64-bit):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[core@bf-13 ~]$ lscpu Architecture: aarch64 Byte Order: Little Endian CPU(s): 8 On-line CPU(s) list: 0-7 Thread(s) per core: 1 Core(s) per socket: 8 Socket(s): 1 NUMA node(s): 1 Vendor ID: ARM Model: 0 Model name: Cortex-A72 Stepping: r1p0 BogoMIPS: 400.00 L1d cache: 32K L1i cache: 48K L2 cache: 1024K L3 cache: 6144K NUMA node0 CPU(s): 0-7 Flags: fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On one BlueField: &lt;/strong&gt;&lt;code&gt;dmidecode&lt;/code&gt; is a tool for dumping a computer's DMI (also called SMBIOS). The tool is exporting a table of contents. Running on the Jetson gives you an error: “No SMBIOS nor DMI entry point found, sorry.” The command is working on Bluefield-2; you can get SMBIOS data from sysfs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[core@bf-14 ~]$ sudo dmidecode | grep -A12 "BIOS Information" BIOS Information Vendor: https://www.mellanox.com Version: BlueField:3.7.0-20-g98daf29 Release Date: Jun 26 2021 ROM Size: 4 MB Characteristics: PCI is supported BIOS is upgradeable Selectable boot is supported Serial services are supported (int 14h) ACPI is supported UEFI is supported BIOS Revision: 3.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On one BlueField: &lt;/strong&gt;We can get the SOC type with the &lt;code&gt;lshw&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[core@bf-14 ~]$ lshw -C system bf-14 description: Computer product: BlueField SoC (Unspecified System SKU) vendor: https://www.mellanox.com version: 1.0.0 serial: Unspecified System Serial Number width: 64 bits capabilities: smbios-3.1.1 dmi-3.1.1 smp configuration: boot=normal family=BlueField sku=Unspecified System SKU uuid=888eecb3-cb1e-40c0-a925-562a7c62ed92 *-pnp00:00 product: PnP device PNP0c02 physical id: 1 capabilities: pnp configuration: driver=system &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;On one BlueField: &lt;/strong&gt;We have 16GB of RAM on this BlueField-2:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[core@bf-14 ~]$ free -h total used free shared buff/cache available Mem: 15Gi 2.3Gi 6.9Gi 193Mi 6.7Gi 10Gi Swap: 0B 0B 0B &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Infrastructure cluster:&lt;/strong&gt; I can get the OpenShift console URL (Figure 5) with an oc command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[egallen@bastion-infrastructure ~]$ oc whoami --show-console https://console-openshift-console.apps.bf2cluster.dev.metalkube.org [egallen@bastion-infrastructure ~]$ host console-openshift-console.apps.bf2cluster.dev.metalkube.org console-openshift-console.apps.bf2cluster.dev.metalkube.org has address 192.168.111.4 &lt;/code&gt;&lt;/pre&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image3_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image3_2.png?itok=8NZA_hBy" width="600" height="255" alt="Nodes in the OpenShift console." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Getting information about nodes in the cluster.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;&lt;strong&gt;Tenant cluster: &lt;/strong&gt;In the &lt;code&gt;testpod1&lt;/code&gt; we are running the &lt;code&gt;iperf&lt;/code&gt; server:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[root@testpod-1 /]# taskset -c 6 iperf3 -s ----------------------------------------------------------- Server listening on 5201 ----------------------------------------------------------- Accepted connection from 10.130.4.132, port 59326 [ 5] local 10.129.5.163 port 5201 connected to 10.130.4.132 port 59328 [ ID] Interval Transfer Bitrate [ 5] 0.00-1.00 sec 2.35 GBytes 20.2 Gbits/sec [ 5] 1.00-2.00 sec 2.46 GBytes 21.2 Gbits/sec [ 5] 2.00-3.00 sec 2.42 GBytes 20.8 Gbits/sec [ 5] 3.00-4.00 sec 2.24 GBytes 19.2 Gbits/sec [ 5] 4.00-5.00 sec 2.39 GBytes 20.5 Gbits/sec [ 5] 5.00-5.00 sec 384 KBytes 21.2 Gbits/sec - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate [ 5] 0.00-5.00 sec 11.9 GBytes 20.4 Gbits/sec receiver ----------------------------------------------------------- Server listening on 5201 ----------------------------------------------------------- &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Tenant cluster: &lt;/strong&gt;In the &lt;code&gt;testpod2&lt;/code&gt;, we can get 20-21.2Gbps of throughput with TC Flower OVS hardware offloading instead of 3Gbps of traffic throughput without offloading:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[root@testpod-2 /]# taskset -c 6 iperf3 -c 10.129.5.163 -t 5 Connecting to host 10.129.5.163, port 5201 [ 5] local 10.130.4.132 port 59328 connected to 10.129.5.163 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 2.35 GBytes 20.2 Gbits/sec 17 1.14 MBytes [ 5] 1.00-2.00 sec 2.46 GBytes 21.2 Gbits/sec 0 1.41 MBytes [ 5] 2.00-3.00 sec 2.42 GBytes 20.8 Gbits/sec 637 1.01 MBytes [ 5] 3.00-4.00 sec 2.24 GBytes 19.2 Gbits/sec 0 1.80 MBytes [ 5] 4.00-5.00 sec 2.39 GBytes 20.6 Gbits/sec 2 1.80 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-5.00 sec 11.9 GBytes 20.4 Gbits/sec 656 sender [ 5] 0.00-5.00 sec 11.9 GBytes 20.4 Gbits/sec receiver iperf Done. &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This example demonstrates the benefits of composable compute architectures that include hardware-level security isolation and capabilities to offload the entire subsystem, such as networking, to the DPU hardware. This enables clean architectural compartmentalization along the same boundaries as the corresponding software services, as well as freeing up working node x83 cores so they can run more applications.&lt;/p&gt; &lt;p&gt;Unlike SmartNICs that have been niche products with proprietary software stacks, DPUs running &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;, Kubernetes, and open source software stacks offer network function portability. And with Red Hat OpenShift, we are offering long-term support for this stack.&lt;/p&gt; &lt;p&gt;As DPUs gain more compute power, additional capabilities, and increased popularity in datacenters around the world, Red Hat and NVIDIA plan to continue work on enabling the offloading of additional software functions to DPUs.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/26/orchestrate-offloaded-network-functions-dpus-red-hat-openshift" title="Orchestrate offloaded network functions on DPUs with Red Hat OpenShift"&gt;Orchestrate offloaded network functions on DPUs with Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Erwan Gallen</dc:creator><dc:date>2022-04-26T10:00:00Z</dc:date></entry><entry><title>Red Hat Developer roundup: Best of April 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/26/red-hat-developer-roundup-best-april-2022" /><author><name>Red Hat Developer Editorial Team</name></author><id>8264f386-94d5-4766-9207-39e82b323299</id><updated>2022-04-26T07:00:00Z</updated><published>2022-04-26T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome to our monthly recap of the articles we published in April! We had some important product and event announcements this month that deserve your attention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/18/announcement-red-hat-codeready-studio-reaches-end-life"&gt;Red Hat CodeReady Studio reached its end of life&lt;/a&gt;;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/01/codeready-workspaces-scales-now-red-hat-openshift-dev-spaces"&gt;CodeReady Workspaces has scaled up and, is now Red Hat OpenShift Dev Spaces&lt;/a&gt;;&lt;/li&gt; &lt;li&gt;Red Hat Summit 2022 is fast approaching—find out which sessions are &lt;a href="https://developers.redhat.com/articles/2022/04/11/red-hat-summit-2022-developer-preview"&gt;of most interest to developers&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And of course, we rolled out a panoply of articles to help you write code on the platforms you trust. Here are the April highlights.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: See the end of this article for the full lineup published in April 2022.&lt;/p&gt; &lt;h2&gt;What's new in GCC 12?&lt;/h2&gt; &lt;p&gt;The upcoming release of version 12 of the GCC &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;compiler&lt;/a&gt; is naturally causing quite a stir for &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt; developers working on &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; and other platforms. In one of our most popular articles this month, David Malcolm breaks down the &lt;a href="https://developers.redhat.com/articles/2022/04/12/state-static-analysis-gcc-12-compiler"&gt;state of static code analysis in GCC 12&lt;/a&gt;. C++ specifically is getting more support in this version of the compiler, and Marek Polacek &lt;a href="https://developers.redhat.com/articles/2022/04/11/new-c-features-gcc-12"&gt;outlines the new features&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Meanwhile, C++ is advancing on other fronts, with updates to the core standard. Jason Merrill &lt;a href="https://developers.redhat.com/articles/2022/03/29/c-standardization-core-language-progress-2021"&gt;has the highlights&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Keep secure with SSO&lt;/h2&gt; &lt;p&gt;Did you know that Red Hat makes single sign-on available in many of its products and platforms? The technology is a productized and supported version of the open source Keycloak tool. This month we offered a couple of different ways to help you get started: Romain Pelisse taught you how to &lt;a href="https://developers.redhat.com/articles/2022/04/20/deploy-keycloak-single-sign-ansible"&gt;deploy Keycloak SSO with Ansible&lt;/a&gt;, while Olivier Rivat explained how to &lt;a href="https://developers.redhat.com/articles/2022/04/21/add-security-quarkus-application-using-red-hats-sso"&gt;add security to a Quarkus application using Red Hat's SSO&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Get to know Node.js&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/nodejs/all"&gt;Node.js&lt;/a&gt; makes it simple to run &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; on servers and everywhere else. Developers at IBM, Red Hat, and elsewhere are working together to create a &lt;a href="https://nodeshift.dev/nodejs-reference-architecture/"&gt;Node.js reference architecture&lt;/a&gt;; in the latest installment in &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;our series on the topic&lt;/a&gt;, Dominic Harries explains what this architecture's &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;recommendations are for TypeScript&lt;/a&gt;. And if you're ready to get your hands dirty with some real-world development, find out how to &lt;a href="https://developers.redhat.com/articles/2022/04/21/bind-kafka-cluster-nodejs-application-easy-way"&gt;bind a Kafka cluster to a Node.js application the easy way&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Java, containerized&lt;/h2&gt; &lt;p&gt;Java might have been born in an era of big servers and traditional middleware, but it's moving forward into an era of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; applications at a rapid clip. This month, Red Hat Developer delivered a pair of articles that dug deep into how Java operates in a containerized environment. Ben Evans outlined some &lt;a href="https://developers.redhat.com/articles/2022/04/19/best-practices-java-single-core-containers"&gt;best practices for Java in single-core containers&lt;/a&gt;, whereas Severin Gehwolf explained &lt;a href="https://developers.redhat.com/articles/2022/04/19/java-17-whats-new-openjdks-container-awareness"&gt;what’s new in OpenJDK's container awareness for Java 17&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Want to move beyond theory to practice? Get started with &lt;a href="https://developers.redhat.com/articles/2022/04/05/developers-guide-using-kafka-java-part-1"&gt;a developer's guide to using Kafka with Java&lt;/a&gt;, since Kafka is often used along with Kubernetes. Then dig deeper by &lt;a href="https://developers.redhat.com/articles/2022/04/13/deploy-java-application-red-hat-openshift-using-jkube"&gt;deploying a Java application on Red Hat OpenShift using JKube&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/2022/04/04/writing-kubernetes-operators-java-josdk-part-3-implementing-controller"&gt;writing Kubernetes Operators in Java with JOSDK&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;April 2022 on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Here's the full lineup of articles published on Red Hat Developer so far this month:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/28/build-your-first-java-serverless-function-using-quarkus-quick-start"&gt;Build your first Java serverless function using a Quarkus quick start&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/29/c-standardization-core-language-progress-2021"&gt;C++ standardization (core language) progress in 2021&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/29/develop-basic-rate-limiter-quarkus-and-redis"&gt;Develop a basic rate limiter with Quarkus and Redis&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/03/30/generate-and-save-html-report-jenkins-openshift-4"&gt;Generate and save an HTML report in Jenkins on OpenShift 4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/01/codeready-workspaces-scales-now-red-hat-openshift-dev-spaces"&gt;CodeReady Workspaces scales up, is now Red Hat OpenShift Dev Spaces&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/04/writing-kubernetes-operators-java-josdk-part-3-implementing-controller"&gt;Writing Kubernetes Operators in Java with JOSDK, Part 3: Implementing a controller&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/05/automate-cicd-pull-requests-argo-cd-applicationsets"&gt;Automate CI/CD on pull requests with Argo CD ApplicationSets&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/05/developers-guide-using-kafka-java-part-1"&gt;A developer's guide to using Kafka with Java, Part 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/06/configure-codeready-containers-aiml-development"&gt;Configure CodeReady Containers for AI/ML development&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/06/introduction-linux-bridging-commands-and-features"&gt;An introduction to Linux bridging commands and features&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/07/3-ways-install-database-helm-charts"&gt;3 ways to install a database with Helm charts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/07/kafka-monthly-digest-march-2022"&gt;Kafka Monthly Digest: March 2022&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;Introduction to the Node.js reference architecture, Part 8: TypeScript&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/11/red-hat-summit-2022-developer-preview"&gt;Red Hat Summit 2022: A developer preview&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/state-static-analysis-gcc-12-compiler"&gt;The state of static analysis in the GCC 12 compiler&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/13/deploy-java-application-red-hat-openshift-using-jkube"&gt;Deploy a Java application on Red Hat OpenShift using JKube&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/13/manage-namespaces-multitenant-clusters-argo-cd-kustomize-and-helm"&gt;Manage namespaces in multitenant clusters with Argo CD, Kustomize, and Helm&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/14/building-quarkus-applications-apache-cassandra-workshop-recap"&gt;Building Quarkus applications with Apache Cassandra: Workshop recap&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/14/generate-helm-charts-your-java-application-using-jkube-part-1"&gt;Generate Helm charts for your Java application using JKube, Part 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/18/announcement-red-hat-codeready-studio-reaches-end-life"&gt;Announcement: Red Hat CodeReady Studio reaches end of life&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/19/best-practices-java-single-core-containers"&gt;Best practices for Java in single-core containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/19/java-17-whats-new-openjdks-container-awareness"&gt;Java 17: What’s new in OpenJDK's container awareness&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/20/create-and-manage-local-persistent-volumes-codeready-containers"&gt;Create and manage local persistent volumes with CodeReady Containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/20/deploy-keycloak-single-sign-ansible"&gt;Deploy Keycloak single sign-on with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/21/bind-kafka-cluster-nodejs-application-easy-way"&gt;Bind a Kafka cluster to a Node.js application the easy way&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/11/new-c-features-gcc-12"&gt;New C++ features in GCC 12&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/26/red-hat-developer-roundup-best-april-2022" title="Red Hat Developer roundup: Best of April 2022"&gt;Red Hat Developer roundup: Best of April 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Red Hat Developer Editorial Team</dc:creator><dc:date>2022-04-26T07:00:00Z</dc:date></entry><entry><title type="html">Extending your configuration with YAML</title><link rel="alternate" href="https://wildfly.org//news/2022/04/26/YAML-configuration-extension/" /><author><name>Emmanuel Hugonnet</name></author><id>https://wildfly.org//news/2022/04/26/YAML-configuration-extension/</id><updated>2022-04-26T05:00:00Z</updated><content type="html">In WildFly the configuration is managed and stored in the standalone.xml. You have several ways to customize your configuration: edit the XML manually (which is not the recommended approach) or create jboss-cli scripts that you can run on each upgrade. So why propose a 'new' solution to customize a server configuration ? Well the idea was to be able to externalize the customization from the 'standard' provided configuration to facilitate server upgrades: just unzip the new release, install/provision your applications and run the same command line. This can also be done with cli scripts that are executed on boot. But those are a bit tricky to write since you have no idempotence on each boot. That’s why we have introduced a new way to do this by using YAML configuration files. The server will be started in read-only mode, that means that you can’t update the configuration and expect your changes to be persisted. Warning Please note that this feature is considered EXPERIMENTAL and thus is DISABLED by default. ACTIVATING THE FEATURE To enable that feature you need to add a ServiceLoader configuration in the org.jboss.as.controller*_ module. You need to create the following file: META-INF/services/org.jboss.as.controller.persistence.ConfigurationExtension containing a single line org.jboss.as.controller.persistence.yaml.YamlConfigurationExtension in the dir folder of the org.jboss.as.controller module. mkdir -p $WILDFLY_HOME/modules/system/layers/base/org/jboss/as/controller/main/dir/META-INF/services/ echo 'org.jboss.as.controller.persistence.yaml.YamlConfigurationExtension' &gt; $WILDFLY_HOME/modules/system/layers/base/org/jboss/as/controller/main/dir/META-INF/services/org.jboss.as.controller.persistence.ConfigurationExtension WRITTING THE YAML Warning Note that the YAML structure doesn’t follow the XML model but . The goal of the YAML files is to be able to customize an existing configuration. It is not here to replace the existing configuration support with XML. As such we won’t support part of the management model. Only those elements would be supported: * core-service * interface * socket-binding-group * subsystem * system-property That means that at least those entries would be ignored: * extension: to add extension to the server as this might require modules which can be missing. * deployment: to add deployments to the server as this require more that just some configuration. * deployment-overlay: to add deployment-overlays to the server as this require more that just some configuration. * path: since those should already have been defined when the YAML files are parsed. The YAML root node must be wildfly-configuration, then you can follow the model tree to add, remove or update resources. If a resource is already present (created by the XML or a previous YAML file) then we will update it, otherwise we will create it. Sample YAML file to define a new PostGresql datasource: wildfly-configuration: subsystem: datasources: jdbc-driver: postgresql: driver-name: postgresql driver-xa-datasource-class-name: org.postgresql.xa.PGXADataSource driver-module-name: org.postgresql.jdbc data-source: PostgreSQLDS: enabled: true exception-sorter-class-name: org.jboss.jca.adapters.jdbc.extensions.postgres.PostgreSQLExceptionSorter jndi-name: java:jboss/datasources/PostgreSQLDS jta: true max-pool-size: 20 min-pool-size: 0 connection-url: "jdbc:postgresql://localhost:5432}/demo" driver-name: postgresql user-name: postgres password: postgres validate-on-match: true background-validation: false background-validation-millis: 10000 flush-strategy: FailingConnectionOnly statistics-enable: false stale-connection-checker-class-name: org.jboss.jca.adapters.jdbc.extensions.novendor.NullStaleConnectionChecker valid-connection-checker-class-name: org.jboss.jca.adapters.jdbc.extensions.postgres.PostgreSQLValidConnectionChecker transaction-isolation: TRANSACTION_READ_COMMITTED As you can see, we are defining a jdbc-driver called postgresql and a data-source called PostgreSQLDS. Note Note that binaries is not managed by the YAML file, you need to create or provision the org.postgresql.jdbc module. OPERATIONS We also provide three operations using tags to provide more flexibility in what you can do with the YAML file. !UNDEFINE: TO UNDEFINE AN ATTRIBUTE Sample YAML file to undefine the CONSOLE logger level: wildfly-configuration: subsystem: logging: console-handler: CONSOLE: level: !undefine !REMOVE: TO REMOVE THE RESOURCE Sample YAML file to remove the embedded Artemis broker and connect to a remote broker: wildfly-configuration: socket-binding-group: standard-sockets: remote-destination-outbound-socket-binding: remote-artemis: host: localhost port: 61616 subsystem: messaging-activemq: server: default: !remove remote-connector: artemis: socket-binding: remote-artemis pooled-connection-factory: RemoteConnectionFactory: connectors: - artemis entries: - "java:jboss/RemoteConnectionFactory" - "java:jboss/exported/jms/RemoteConnectionFactory" enable-amq1-prefix: false user: admin password: admin ejb3: default-resource-adapter-name: RemoteConnectionFactory ee: service: default-bindings: jms-connection-factory: "java:jboss/RemoteConnectionFactory" !LIST-ADD: TO ADD AN ELEMENT TO A LIST (WITH AN OPTIONNAL INDEX). Sample YAML file to add a RemoteTransactionPermission to the permissions list at the position 0: wildfly-configuration: subsystem: elytron: permission-set: default-permissions: permissions: !list-add - class-name: org.wildfly.transaction.client.RemoteTransactionPermission module: org.wildfly.transaction.client target-name: "*" index: 0 As you may have noticed the index attribute doesn’t exist. It is used to know where to place the entry. If none is defined then the entry will be appended to the list. STARTING WITH YAML FILES Using the --yaml or -y argument you can pass a list of YAML files. Each path needs to be separated by the File.pathSeparator. It is a semicolon (;) on Windows and colon (:) on Mac and Unix-based operating systems. Paths can be absolute, relative to the current execution directory or relative to the standalone configuration directory. ./standalone.sh -y=/home/ehsavoie/dev/wildfly/config2.yml:config.yml -c standalone-full.xml YouTube video player</content><dc:creator>Emmanuel Hugonnet</dc:creator></entry></feed>
